#!/usr/bin/env python3
# encoding: utf-8

__author__ = "ChenyangGao <https://chenyanggao.github.io>"
__version__ = (0, 0, 7)
__all__ = ["updatedb", "updatedb_one", "updatedb_tree"]
__doc__ = "éå† 115 ç½‘ç›˜çš„ç›®å½•ä¿¡æ¯å¯¼å‡ºåˆ°æ•°æ®åº“"
__requirements__ = ["p115client", "posixpatht", "urllib3", "urllib3_request"]

if __name__ == "__main__":
    from argparse import ArgumentParser, RawTextHelpFormatter

    parser = ArgumentParser(
        formatter_class=RawTextHelpFormatter, 
        description=__doc__, 
    )
    parser.add_argument("top_dirs", metavar="dir", nargs="*", help="""\
115 ç›®å½•ï¼Œå¯ä»¥ä¼ å…¥å¤šä¸ªï¼Œå¦‚æœä¸ä¼ é»˜è®¤ä¸º 0
å…è®¸ 3 ç§ç±»å‹çš„ç›®å½•
    1. æ•´æ•°ï¼Œè§†ä¸ºç›®å½•çš„ id
    2. å½¢å¦‚ "/åå­—/åå­—/..." çš„è·¯å¾„ï¼Œæœ€å‰é¢çš„ "/" å¯ä»¥çœç•¥ï¼Œæœ¬ç¨‹åºä¼šå°è¯•è·å–å¯¹åº”çš„ id
    3. å½¢å¦‚ "æ ¹ç›®å½• > åå­— > åå­— > ..." çš„è·¯å¾„ï¼Œæ¥è‡ªç‚¹å‡»æ–‡ä»¶çš„ã€æ˜¾ç¤ºå±æ€§ã€‘ï¼Œåœ¨ã€ä½ç½®ã€‘è¿™éƒ¨åˆ†çœ‹åˆ°çš„è·¯å¾„ï¼Œæœ¬ç¨‹åºä¼šå°è¯•è·å–å¯¹åº”çš„ id
""")
    parser.add_argument("-c", "--cookies", help="115 ç™»å½• cookiesï¼Œä¼˜å…ˆçº§é«˜äº -cp/--cookies-path")
    parser.add_argument("-cp", "--cookies-path", help="""\
å­˜å‚¨ 115 ç™»å½• cookies çš„æ–‡æœ¬æ–‡ä»¶çš„è·¯å¾„ï¼Œå¦‚æœç¼ºå¤±ï¼Œåˆ™ä» 115-cookies.txt æ–‡ä»¶ä¸­è·å–ï¼Œæ­¤æ–‡ä»¶å¯åœ¨å¦‚ä¸‹ç›®å½•ä¹‹ä¸€: 
    1. å½“å‰å·¥ä½œç›®å½•
    2. ç”¨æˆ·æ ¹ç›®å½•
    3. æ­¤è„šæœ¬æ‰€åœ¨ç›®å½•
å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œåˆ™é»˜è®¤ä½¿ç”¨ '2. ç”¨æˆ·æ ¹ç›®å½•ï¼Œæ­¤æ—¶åˆ™éœ€è¦æ‰«ç ç™»å½•'""")
    parser.add_argument("-f", "--dbfile", default="", help="sqlite æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºåœ¨å½“å‰å·¥ä½œç›®å½•ä¸‹çš„ f'115-{user_id}.db'")
    parser.add_argument("-cl", "--clean", action="store_true", help="ä»»åŠ¡å®Œæˆåæ¸…ç†æ•°æ®åº“ï¼Œä»¥èŠ‚çº¦ç©ºé—´")
    parser.add_argument("-st", "--auto-splitting-threshold", type=int, default=200_000, help="è‡ªåŠ¨æ‹†åˆ†çš„æ–‡ä»¶æ•°é˜ˆå€¼ï¼Œå¤§äºæ­¤å€¼æ—¶ï¼Œè‡ªåŠ¨è¿›è¡Œæ‹†åˆ†ï¼Œå¦‚æœ <= 0ï¼Œåˆ™æ€»æ˜¯æ‹†åˆ†ï¼Œé»˜è®¤å€¼ 200,000ï¼ˆ20 ä¸‡ï¼‰")
    parser.add_argument("-sst", "--auto-splitting-statistics-timeout", type=float, default=3, help="è‡ªåŠ¨æ‹†åˆ†å‰çš„æ‰§è¡Œæ–‡ä»¶æ•°ç»Ÿè®¡çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¤§äºæ­¤å€¼æ—¶ï¼Œè§†ä¸ºæ–‡ä»¶æ•°æ— ç©·å¤§ï¼Œå¦‚æœ <= 0ï¼Œè§†ä¸ºæ°¸ä¸è¶…æ—¶ï¼Œé»˜è®¤å€¼ 3")
    parser.add_argument("-nm", "--no-dir-moved", action="store_true", help="å£°æ˜æ²¡æœ‰ç›®å½•è¢«ç§»åŠ¨æˆ–æ”¹åï¼ˆä½†å¯ä»¥æœ‰ç›®å½•è¢«æ–°å¢æˆ–åˆ é™¤ï¼‰ï¼Œè¿™å¯ä»¥åŠ å¿«æ‰¹é‡æ‹‰å–æ—¶çš„é€Ÿåº¦")
    parser.add_argument("-nr", "--not-recursive", action="store_true", help="ä¸éå†ç›®å½•æ ‘ï¼šåªæ‹‰å–é¡¶å±‚ç›®å½•ï¼Œä¸é€’å½’å­ç›®å½•")
    parser.add_argument("-v", "--version", action="store_true", help="è¾“å‡ºç‰ˆæœ¬å·")

    args = parser.parse_args()
    if args.version:
        print(".".join(map(str, __version__)))
        raise SystemExit(0)

try:
    from p115client import check_response, P115Client
    from p115client.exception import BusyOSError
    from p115client.tool.iterdir import ensure_attr_path, iter_stared_dirs, DirNode, DirNodeTuple
    from posixpatht import escape, joins, normpath
    from urllib3.poolmanager import PoolManager
    from urllib3.exceptions import ReadTimeoutError
    from urllib3_request import request as urllib3_request
except ImportError:
    from sys import executable
    from subprocess import run
    run([executable, "-m", "pip", "install", "-U", *__requirements__], check=True)
    from p115client import check_response, P115Client
    from p115client.exception import BusyOSError
    from p115client.tool.iterdir import ensure_attr_path, iter_stared_dirs, DirNode, DirNodeTuple
    from posixpatht import escape, joins, normpath
    from urllib3.poolmanager import PoolManager
    from urllib3.exceptions import ReadTimeoutError
    from urllib3_request import request as urllib3_request

import logging

from collections import deque
from collections.abc import Collection, Iterator, Iterable, Mapping, Sequence, Set
from contextlib import contextmanager
from errno import EBUSY, ENOENT, ENOTDIR
from functools import partial
from itertools import islice, takewhile
from math import isnan, isinf
from sqlite3 import connect, Connection, Cursor
from time import time
from typing import cast, Final


# NOTE: ç›®å½•çš„ id åˆ°å®ƒçš„ åå­— å’Œ ä¸Šçº§ç›®å½• id çš„æ˜ å°„
ID_TO_DIRNODE: Final[dict[int, DirNode | DirNodeTuple]] = {}
# NOTE: åˆ›å»ºä¸€ä¸ªä½¿ç”¨ urllib3 çš„è¯·æ±‚å‡½æ•°ï¼Œè¿æ¥æ± å®¹é‡ä¸º 50
request = partial(urllib3_request, pool=PoolManager(50))
# NOTE: åˆå§‹åŒ–æ—¥å¿—å¯¹è±¡
logger = logging.Logger("115-updatedb", level=logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter(
    "[\x1b[1m%(asctime)s\x1b[0m] (\x1b[1;36m%(levelname)s\x1b[0m) "
    "\x1b[0m\x1b[1;35m%(name)s\x1b[0m \x1b[5;31mâœ\x1b[0m %(message)s"
))
logger.addHandler(handler)


def normalize_path(path: str, /) -> int | str:
    """è§„èŒƒåŒ–è·¯å¾„

    :param path: è·¯å¾„ï¼Œè·¯å¾„å¯ä»¥æ˜¯å¤šç§å½¢å¼

        1. è‡ªç„¶æ•°ï¼Œè§†ä¸º id
        2. å½¢å¦‚ "/åå­—/åå­—/..." çš„è·¯å¾„ï¼Œæœ€å‰é¢çš„ "/" å¯ä»¥çœç•¥
        3. å½¢å¦‚ "æ ¹ç›®å½• > åå­— > åå­— > ..." çš„è·¯å¾„ï¼Œæ¥è‡ªç‚¹å‡»æ–‡ä»¶çš„ã€æ˜¾ç¤ºå±æ€§ã€‘ï¼Œåœ¨ã€ä½ç½®ã€‘è¿™éƒ¨åˆ†çœ‹åˆ°çš„è·¯å¾„

    :return: è§£æè·¯å¾„ï¼Œè¿”å›ç›¸åº”å€¼

        - å¦‚æœå¯ä»¥è¢«è§£æä¸ºæ•´æ•°ï¼Œåˆ™è¿”å›æ­¤æ•°ï¼Œä½œä¸º id
        - å¦‚æœè¢«è§£æä¸ºæ ¹ç›®å½• "/"ï¼Œåˆ™è¿”å› 0
        - å¦åˆ™ï¼Œå¯¹è·¯å¾„è¿›è¡Œä¸€äº›æ•´ç†ï¼Œå¹¶è§£æ "." å’Œ  ".."ï¼Œç„¶åè¿”å›è§£æåçš„è·¯å¾„
    """
    if path in ("", "0", ".", "..", "/"):
        return 0
    if not path.startswith("0") and path.isdecimal():
        return int(path)
    if path.startswith("æ ¹ç›®å½• > "):
        patht = path.split(" > ")
        if len(patht) == 1:
            return 0
        patht[0] = ""
        return joins(patht)
    path = normpath("/" + path)
    if path == "/":
        return 0
    return path


def normalize_attr(info: Mapping, /) -> dict:
    """ç­›é€‰å’Œè§„èŒƒåŒ–æ•°æ®çš„åå­—ï¼Œä»¥ä¾¿æ’å…¥ `data` è¡¨

    :param info: åŸå§‹æ•°æ®

    :return: ç»è¿‡è§„èŒƒåŒ–åçš„æ•°æ®
    """
    is_dir = "fid" not in info
    if is_dir:
        attr: dict = {"id": int(info["cid"]), "parent_id": int(info["pid"])}
    else:
        attr = {"id": int(info["fid"]), "parent_id": int(info["cid"])}
    attr["pickcode"] = info["pc"]
    attr["name"] = info["n"]
    attr["size"] = info.get("s") or 0
    attr["sha1"] = info.get("sha") or ""
    attr["is_dir"] = is_dir
    attr["is_image"] = not is_dir and bool(info.get("u"))
    attr["ctime"] = int(info.get("tp", 0))
    attr["mtime"] = int(info.get("te", 0))
    return attr


def normalize_dir_attr(info: Mapping, /) -> dict:
    """ç­›é€‰å’Œè§„èŒƒåŒ–æ•°æ®çš„åå­—ï¼Œä»¥ä¾¿æ’å…¥ `dir` è¡¨

    :param info: åŸå§‹æ•°æ®

    :return: ç»è¿‡è§„èŒƒåŒ–åçš„æ•°æ®
    """
    return {
        "id": int(info["cid"]), 
        "parent_id": int(info["pid"]), 
        "pickcode": info["pc"], 
        "name": info["n"], 
        "ctime": int(info["tp"]), 
        "mtime": int(info["te"]), 
    }


def get_dir_path(cid: int = 0, /) -> str:
    """ç”±ç›®å½•çš„ id è·å–å®ƒçš„ è·¯å¾„

    :param cid: ç›®å½•çš„ id

    :return: ç›®å½•çš„è·¯å¾„
    """
    if not cid:
        return "/"
    parts: list[str] = []
    add = parts.append
    while cid:
        name, cid = ID_TO_DIRNODE[cid]
        add(escape(name))
    add("")
    return "/".join(reversed(parts))


@contextmanager
def transaction(con: Connection | Cursor, /):
    """æ‰§è¡Œä¸€æ¬¡æ•°æ®åº“æäº¤ï¼ˆcommitï¼‰

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    """
    if isinstance(con, Cursor):
        con = con.connection
    try:
        yield
    except:
        con.rollback()
        raise
    else:
        con.commit()


def update_desc(
    client: P115Client, 
    ids: Iterable[int], 
    /, 
    desc: str = "", 
    batch_size: int = 10_000, 
):
    """è®¾ç½®æ–‡ä»¶æˆ–ç›®å½•çš„å¤‡æ³¨ä¸ºç©ºï¼Œæ­¤ä¸¾å¯æ›´æ–°æ­¤æ–‡ä»¶æˆ–ç›®å½•çš„ mtime

    :param client: 115 ç½‘ç›˜å®¢æˆ·ç«¯å¯¹è±¡
    :param ids: ä¸€ç»„æ–‡ä»¶æˆ–ç›®å½•çš„ id
    :param batch_size: æ‰¹æ¬¡å¤§å°ï¼Œåˆ†æ‰¹æ¬¡ï¼Œæ¯æ¬¡æäº¤çš„ id æ•°
    """
    set_desc = client.fs_desc_set
    if isinstance(ids, Sequence):
        for i in range(0, len(ids), batch_size):
            check_response(set_desc(ids[i:i+batch_size], desc, request=request))
    else:
        ids_it = iter(ids)
        while t_ids := tuple(islice(ids_it, batch_size)):
            check_response(set_desc(t_ids, desc, request=request))


def update_star(
    client: P115Client, 
    ids: Iterable[int], 
    /, 
    star: bool = True, 
    batch_size: int = 10_000, 
):
    """ç»™æ–‡ä»¶æˆ–ç›®å½•åŠ ä¸Šæ˜Ÿæ ‡ï¼Œæ­¤ä¸¾å°±ç›®å½•è€Œè¨€ï¼Œå¯ä»¥å®ç°æ‰¹é‡æ‹‰å–

    :param client: 115 ç½‘ç›˜å®¢æˆ·ç«¯å¯¹è±¡
    :param ids: ä¸€ç»„æ–‡ä»¶æˆ–ç›®å½•çš„ id
    :param batch_size: æ‰¹æ¬¡å¤§å°ï¼Œåˆ†æ‰¹æ¬¡ï¼Œæ¯æ¬¡æäº¤çš„ id æ•°
    """
    set_star = client.fs_star_set
    if isinstance(ids, Sequence):
        for i in range(0, len(ids), batch_size):
            idss = ",".join(map(str, ids[i:i+batch_size]))
            check_response(set_star(idss, star, request=request))
    else:
        ids_it = iter(ids)
        while idss := ",".join(map(str, islice(ids_it, batch_size))):
            check_response(set_star(idss, star, request=request))


def filter_na_ids(
    client: P115Client, 
    ids: Iterable[int], 
    /, 
    batch_size: int = 50_000, 
) -> Iterator[int]:
    """æ‰¾å‡ºä¸€ç»„ id ä¸­æ— æ•ˆçš„ï¼Œæ‰€è°“æ— æ•ˆå°±æ˜¯æŒ‡ä¸åœ¨ç½‘ç›˜ä¸­ï¼Œå¯èƒ½å·²ç»è¢«åˆ é™¤ï¼Œä¹Ÿå¯èƒ½ä»æœªå­˜åœ¨è¿‡

    :param client: 115 ç½‘ç›˜å®¢æˆ·ç«¯å¯¹è±¡
    :param ids: ä¸€ç»„æ–‡ä»¶æˆ–ç›®å½•çš„ id
    :param batch_size: æ‰¹æ¬¡å¤§å°ï¼Œåˆ†æ‰¹æ¬¡ï¼Œæ¯æ¬¡æäº¤çš„ id æ•°

    :return: è¿­ä»£å™¨ï¼Œç­›é€‰å‡ºæ‰€æœ‰æ— æ•ˆçš„ id
    """
    def check_part(ids: Iterable[int], /) -> Iterable[int]:
        resp = client.fs_file_skim(ids, request=request)
        if resp.get("error") == "æ–‡ä»¶ä¸å­˜åœ¨":
            return ids
        else:
            check_response(resp)
            if not isinstance(ids, Set):
                ids = set(ids)
            return ids - {int(a["file_id"]) for a in resp["data"]}
    if isinstance(ids, Sequence):
        for i in range(0, len(ids), batch_size):
            yield from check_part(ids[i:i+batch_size])
    else:
        ids_it = iter(ids)
        while t_ids := tuple(islice(ids_it, batch_size)):
            yield from check_part(t_ids)


def execute_commit(
    con: Connection | Cursor, 
    /, 
    sql: str, 
    params = None, 
    executemany: bool = False, 
) -> Cursor:
    """æ‰§è¡Œä¸€ä¸ª sql è¯­å¥ï¼Œå¹¶è‡ªåŠ¨æäº¤ï¼ˆcommitï¼‰å’Œå›æ»šï¼ˆrollbackï¼‰

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param sql: sql è¯­å¥
    :param params: å‚æ•°ï¼Œç”¨äºå¡«å…… sql ä¸­çš„å ä½ç¬¦
    :param executemany: å¦‚æœä¸º Trueï¼Œåˆ™æ‰§è¡Œ `.executemany(sql, params)`ï¼Œå¦åˆ™æ‰§è¡Œ `.execute(sql, params)`

    :return: æ¸¸æ ‡
    """
    if isinstance(con, Connection):
        cur = con.cursor()
    else:
        cur = con
        con = cur.connection
    try:
        if executemany:
            cur = con.executemany(sql, params)
        elif params is None:
            cur = con.execute(sql)
        else:
            cur = con.execute(sql, params)
        con.commit()
        return cur
    except BaseException:
        con.rollback()
        raise


def initdb(con: Connection | Cursor, /) -> Cursor:
    """åˆå§‹åŒ–æ•°æ®åº“ï¼Œä¼šå°è¯•åˆ›å»ºä¸€äº›è¡¨ã€ç´¢å¼•ã€è§¦å‘å™¨ã€æ‰©å±•å‡½æ•°ç­‰ï¼Œå¹¶æŠŠè¡¨çš„ "journal_mode" æ”¹ä¸º WAL (write-ahead-log)

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡

    :return: æ¸¸æ ‡
    """
    conn: Connection = con.connection if isinstance(con, Cursor) else con
    conn.create_function("escape_name", 1, escape)
    return con.executescript("""\
-- ä¿®æ”¹æ—¥å¿—æ¨¡å¼ä¸º WAL (write-ahead-log)
PRAGMA journal_mode = WAL;

-- åˆ›å»º data è¡¨
CREATE TABLE IF NOT EXISTS data (
    id INTEGER NOT NULL PRIMARY KEY,   -- æ–‡ä»¶æˆ–ç›®å½•çš„ id
    parent_id INTEGER NOT NULL,        -- ä¸Šçº§ç›®å½•çš„ id
    pickcode TEXT NOT NULL DEFAULT '', -- æå–ç ï¼Œä¸‹è½½æ—¶éœ€è¦ç”¨åˆ°
    name TEXT NOT NULL,                -- åå­—
    size INTEGER NOT NULL DEFAULT 0,   -- æ–‡ä»¶å¤§å°
    sha1 TEXT NOT NULL DEFAULT '',     -- æ–‡ä»¶çš„ sha1 æ•£åˆ—å€¼
    is_dir INTEGER NOT NULL CHECK(is_dir IN (0, 1)), -- æ˜¯å¦ç›®å½•
    is_image INTEGER NOT NULL CHECK(is_image IN (0, 1)) DEFAULT 0, -- æ˜¯å¦å›¾ç‰‡
    ctime INTEGER NOT NULL DEFAULT 0,  -- åˆ›å»ºæ—¶é—´æˆ³ï¼Œä¸€æ—¦è®¾ç½®å°±ä¸ä¼šæ›´æ–°
    mtime INTEGER NOT NULL DEFAULT 0,  -- æ›´æ–°æ—¶é—´æˆ³ï¼Œå¦‚æœåå­—ã€å¤‡æ³¨è¢«è®¾ç½®ï¼ˆå³ä½¿å€¼æ²¡å˜ï¼‰ï¼Œæˆ–è€…è¿›å‡ºå›æ”¶ç«™ï¼Œæˆ–è€…ï¼ˆå¦‚æœè‡ªå·±æ˜¯ç›®å½•ï¼‰å¢åˆ ç›´æ¥å­èŠ‚ç‚¹æˆ–è®¾ç½®å°é¢ï¼Œä¼šæ›´æ–°æ­¤å€¼ï¼Œä½†ç§»åŠ¨å¹¶ä¸æ›´æ–°
    path TEXT NOT NULL DEFAULT '',     -- è·¯å¾„
    updated_at DATETIME DEFAULT (strftime('%Y-%m-%dT%H:%M:%S.%f+08:00', 'now', '+8 hours')) -- æœ€è¿‘ä¸€æ¬¡æ›´æ–°æ—¶é—´
);

-- åˆ›å»º dir è¡¨ï¼Œç”¨æ¥å­˜å‚¨æ‰€æœ‰çœ‹åˆ°çš„ç›®å½•æ•°æ®ï¼Œåªå¢æ”¹è€Œä¸åˆ 
CREATE TABLE IF NOT EXISTS dir (
    id INTEGER NOT NULL PRIMARY KEY,   -- ç›®å½•çš„ id
    parent_id INTEGER NOT NULL,        -- ä¸Šçº§ç›®å½•çš„ id
    pickcode TEXT NOT NULL DEFAULT '', -- æå–ç 
    name TEXT NOT NULL,                -- åå­—
    ctime INTEGER NOT NULL DEFAULT 0,  -- åˆ›å»ºæ—¶é—´æˆ³ï¼Œä¸€æ—¦è®¾ç½®å°±ä¸ä¼šæ›´æ–°
    mtime INTEGER NOT NULL DEFAULT 0   -- æ›´æ–°æ—¶é—´æˆ³ï¼Œå¦‚æœåå­—ã€å¤‡æ³¨è¢«è®¾ç½®ï¼ˆå³ä½¿å€¼æ²¡å˜ï¼‰ï¼Œæˆ–è€…è¿›å‡ºå›æ”¶ç«™ï¼Œæˆ–è€…å¢åˆ ç›´æ¥å­èŠ‚ç‚¹ï¼Œæˆ–è€…è®¾ç½®å°é¢ï¼Œä¼šæ›´æ–°æ­¤å€¼ï¼Œä½†ç§»åŠ¨å¹¶ä¸æ›´æ–°
);

-- ç»™ data è¡¨åˆ›å»ºè§¦å‘å™¨ï¼Œè‡ªåŠ¨æ›´æ–° updated_atï¼Œè¿™ä¸ªå­—æ®µè®°å½•æœ€è¿‘ä¸€æ¬¡æ›´æ–°æ—¶é—´
CREATE TRIGGER IF NOT EXISTS trg_data_updated_at
AFTER UPDATE ON data 
FOR EACH ROW
BEGIN
    UPDATE data SET updated_at = strftime('%Y-%m-%dT%H:%M:%S.%f+08:00', 'now', '+8 hours') WHERE id = NEW.id;
END;

-- åˆ›å»ºç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_data_parent_id ON data(parent_id);
CREATE INDEX IF NOT EXISTS idx_data_path ON data(path);
CREATE INDEX IF NOT EXISTS idx_dir_mtime ON dir(mtime);
""")


def select_parent_ids(
    con: Connection | Cursor, 
    ids: Iterable[int], 
    /, 
) -> list[int]:
    """è·å–ä¸€ç»„ id å¯¹åº”çš„ parent_idï¼Œç»è¿‡å»é‡

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param ids: ä¸€ç»„ id

    :return: ä¸€ç»„ id, å·²ç»å»é‡
    """
    sql = "SELECT DISTINCT parent_id FROM data WHERE id IN (%s)" % (",".join(map(str, ids)) or "NULL")
    return [row[0] for row in con.execute(sql)]


def select_subtree_ids(
    con: Connection | Cursor, 
    /, 
    root: int | str = 0, 
) -> list[int]:
    """è·å–ä»¥ `root` ä¸ºæ ¹çš„ç›®å½•æ ‘çš„æ‰€æœ‰èŠ‚ç‚¹çš„ id

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param root: æ ¹èŠ‚ç‚¹çš„ id æˆ– è·¯å¾„

    :return: ä¸€ç»„ id
    """
    if isinstance(root, int):
        sql = """\
WITH RECURSIVE t(id) AS (
    SELECT :root
    UNION ALL
    SELECT data.id FROM t JOIN data WHERE (data.parent_id = t.id)
)
SELECT * FROM t
"""
    else:
        sql = """\
SELECT id 
FROM data 
WHERE path = :root OR path LIKE :root || '/%'
"""
    return [row[0] for row in con.execute(sql, {"root": root})]


def select_subdir_ids(
    con: Connection | Cursor, 
    parent_id: int = 0, 
    /, 
) -> list[int]:
    """è·å–æŸä¸ªç›®å½•ä¹‹ä¸‹çš„æ‰€æœ‰å­ç›®å½•çš„ id

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param parent_id: çˆ¶ç›®å½•çš„ id

    :return: ä¸€ç»„ id
    """
    sql = "SELECT id FROM data WHERE parent_id=? AND is_dir=1"
    return [row[0] for row in con.execute(sql, (parent_id,))]


def select_mtime_groups(
    con: Connection | Cursor, 
    parent_id: int = 0, 
    /, 
    tree: bool = False, 
) -> dict[int, set[int]]:
    """è·å–æŸä¸ªç›®å½•ä¹‹ä¸‹çš„èŠ‚ç‚¹ï¼ˆä¸å«æ­¤èŠ‚ç‚¹æœ¬èº«ï¼‰ï¼ŒæŒ‰ mtime è¿›è¡Œåˆ†ç»„ï¼Œç›¸åŒ mtime çš„ id å½’å…¥åŒä¸€ç»„

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param parent_id: çˆ¶ç›®å½•çš„ id
    :param tree: æ˜¯å¦æ‹‰å–ç›®å½•æ ‘ï¼Œå¦‚æœä¸º Trueï¼Œåˆ™æ‹‰å–å…¨éƒ¨åä»£çš„æ–‡ä»¶èŠ‚ç‚¹ï¼ˆä¸å«ç›®å½•èŠ‚ç‚¹ï¼‰ï¼Œå¦‚æœä¸º Falseï¼Œåˆ™åªæ‹‰å–å­èŠ‚ç‚¹ï¼ˆå«ç›®å½•èŠ‚ç‚¹ï¼‰

    :return: å­—å…¸ï¼Œè¡¨ç¤ºç›¸åŒ mtime çš„ id çš„é›†åˆï¼Œæ‰€ä»¥ key æ˜¯ mtimeï¼Œvalue æ˜¯ä¸€ç»„ id çš„é›†åˆ
    """
    if tree:
        sql = """\
WITH RECURSIVE t AS (
    SELECT id, mtime, is_dir
    FROM data
    WHERE parent_id=?
    UNION ALL
    SELECT data.id, data.mtime, data.is_dir
    FROM t JOIN data ON (data.parent_id = t.id)
)
SELECT mtime, id
FROM t
WHERE is_dir = 0
ORDER BY mtime DESC
"""
    else:
        sql = """\
SELECT mtime, id
FROM data
WHERE parent_id=? AND mtime != 0
ORDER BY mtime DESC
"""
    s: set[int]
    d: dict[int, set[int]] = {}
    add = set.add
    last_mtime = 0
    for mtime, id in con.execute(sql, (parent_id,)):
        if last_mtime == mtime:
            add(s, id)
        else:
            s = d[mtime] = {id}
            last_mtime = mtime
    return d


def select_dangling_ids(
    con: Connection | Cursor, 
    /, 
) -> set[int]:
    """æ‰¾å‡ºæ‰€æœ‰çš„æ‚¬ç©ºèŠ‚ç‚¹çš„ id

    .. note::
        æ‚¬ç©ºèŠ‚ç‚¹ï¼Œå°±æ˜¯æ­¤èŠ‚ç‚¹æœ‰ä¸€ä¸ªç¥–å…ˆèŠ‚ç‚¹çš„ idï¼Œä¸ä¸º 0 ä¸”ä¸åœ¨ `data` è¡¨ä¸­

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡

    :return: ä¸€ç»„æ‚¬ç©ºèŠ‚ç‚¹çš„ id çš„é›†åˆ
    """
    d = dict(con.execute("SELECT id, parent_id FROM data"))
    temp: list[int] = []
    ok_ids: set[int] = set()
    na_ids: set[int] = set()
    push = temp.append
    clear = temp.clear
    update_ok = ok_ids.update
    update_na = na_ids.update
    for k, v in d.items():
        try:
            push(k)
            while k := d[k]:
                if k in ok_ids:
                    update_ok(temp)
                    break
                elif k in na_ids:
                    update_na(temp)
                    break
                push(k)
            else:
                update_ok(temp)
        except KeyError:
            update_na(temp)
        finally:
            clear()
    return na_ids


def select_items_from_dir(
    con: Connection | Cursor, 
    ids: Iterable[int], 
    /, 
) -> list[dict]:
    """ä½¿ç”¨ä¸€ç»„ç›®å½•çš„ id ä» `dir` è¡¨æŸ¥è¯¢å¯¹åº”çš„æ•°æ®

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param ids: ä¸€ç»„ id

    :return: ä¸€ç»„æ•°æ®ï¼Œå¯¹åº” `ids`
    """
    fields = ("id", "parent_id", "pickcode", "name", "ctime", "mtime", "size", "sha1", "is_dir", "is_image")
    sql = """\
SELECT id, parent_id, pickcode, name, ctime, mtime, 0 AS size, '' AS sha1, 1 AS is_dir, 0 AS is_image 
FROM dir WHERE id in (%s)""" % (",".join(map(str, ids)) or "NULL")
    return [dict(zip(fields, row)) for row in con.execute(sql)]


def insert_items(
    con: Connection | Cursor, 
    items: Mapping | Iterable[Mapping], 
    /, 
    commit: bool = True, 
) -> Cursor:
    """å‘ `data` è¡¨æ’å…¥ä¸€ç»„æ•°æ®

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param items: ä¸€ç»„æ•°æ®
    :param commit: æ˜¯å¦æäº¤

    :return: æ¸¸æ ‡
    """
    sql = """\
INSERT INTO
    data(id, parent_id, pickcode, name, size, sha1, is_dir, is_image, ctime, mtime, path)
VALUES
    (:id, :parent_id, :pickcode, :name, :size, :sha1, :is_dir, :is_image, :ctime, :mtime, :path)
ON CONFLICT(id) DO UPDATE SET
    parent_id = excluded.parent_id,
    name      = CASE WHEN is_dir THEN name ELSE excluded.name END,
    mtime     = excluded.mtime,
    path      = excluded.path
"""
    if isinstance(items, Mapping):
        items = items,
    if commit:
        return execute_commit(con, sql, items, executemany=True)
    else:
        return con.executemany(sql, items)


def insert_dir_items(
    con: Connection | Cursor, 
    items: Mapping | Iterable[Mapping], 
    /, 
    commit: bool = True, 
) -> Cursor:
    """å‘ `dir` è¡¨æ’å…¥ä¸€ç»„æ•°æ®

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param items: ä¸€ç»„æ•°æ®
    :param commit: æ˜¯å¦æäº¤

    :return: æ¸¸æ ‡
    """
    sql = """\
INSERT INTO
    dir(id, parent_id, pickcode, name, ctime, mtime)
VALUES
    (:id, :parent_id, :pickcode, :name, :ctime, :mtime)
ON CONFLICT(id) DO UPDATE SET
    parent_id = excluded.parent_id,
    pickcode  = excluded.pickcode,
    name      = excluded.name,
    ctime     = excluded.ctime,
    mtime     = excluded.mtime
"""
    if isinstance(items, Mapping):
        items = items,
    if commit:
        return execute_commit(con, sql, items, executemany=True)
    else:
        return con.executemany(sql, items)


def insert_dir_incomplete_items(
    con: Connection | Cursor, 
    items: Mapping | Iterable[Mapping], 
    /, 
    commit: bool = True, 
) -> Cursor:
    """å‘ `dir` è¡¨æ’å…¥ä¸€ç»„æ•°æ®ï¼Œåªä½¿ç”¨æ•°æ®ä¸­ "id"ã€"name"ã€"parent_id" å­—æ®µ

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param items: ä¸€ç»„æ•°æ®
    :param commit: æ˜¯å¦æäº¤

    :return: æ¸¸æ ‡
    """
    sql = """\
INSERT INTO
    dir(id, parent_id, name)
VALUES
    (:id, :parent_id, :name)
ON CONFLICT(id) DO UPDATE SET
    parent_id = excluded.parent_id,
    name      = excluded.name
"""
    if isinstance(items, Mapping):
        items = items,
    if commit:
        return execute_commit(con, sql, items, executemany=True)
    else:
        return con.executemany(sql, items)


def delete_items(
    con: Connection | Cursor, 
    ids: int | Iterable[int], 
    /, 
    commit: bool = True, 
) -> tuple[Cursor, int]:
    """ä½¿ç”¨ id å»ç­›é€‰å’Œåˆ é™¤ä¸€ç»„æ•°æ®

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param ids: ä¸€ç»„ idï¼Œä¼šè¢«åˆ é™¤
    :param commit: æ˜¯å¦æäº¤

    :return: æ¸¸æ ‡ å’Œ åˆ é™¤çš„æ•°æ®é‡
    """
    if isinstance(ids, int):
        cond = f"id = {ids:d}"
    else:
        cond = "id IN (%s)" % (",".join(map(str, ids)) or "NULL")
    sql = "DELETE FROM data WHERE " + cond
    if commit:
        cur = execute_commit(con, sql)
    else:
        cur = con.execute(sql)
    return cur, cur.rowcount


def delete_dangling_items(
    con: Connection | Cursor, 
    /, 
    commit: bool = True, 
) -> tuple[Cursor, int]:
    """åˆ é™¤æ‰€æœ‰çš„æ‚¬ç©ºèŠ‚ç‚¹

    .. note::
        æ‰€è°“æ‚¬ç©ºï¼Œæ„æŒ‡é€šè¿‡ paren_id å­—æ®µå¾€ä¸Šæ‰¾å¯»ï¼Œå­˜åœ¨æŸä¸ª paren_id != 0 ä¸”ä¸åœ¨æ•°æ®åº“ä¸­

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param commit: æ˜¯å¦æäº¤

    :return: æ¸¸æ ‡ å’Œ åˆ é™¤çš„æ•°æ®é‡
    """
    return delete_items(con, select_dangling_ids(con), commit=commit)


def delete_na_dirs(
    con, 
    /, 
    client: P115Client, 
    commit: bool = True, 
) -> tuple[Cursor, int]:
    """åˆ é™¤æ— æ•ˆçš„ç›®å½•ï¼Œä¹Ÿå°±æ˜¯æ•°æ®åº“ä¸­å­˜åœ¨ï¼Œä½†æ˜¯ç½‘ç›˜ä¸­ä¸å­˜åœ¨çš„ç›®å½•

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param client: 115 ç½‘ç›˜å®¢æˆ·ç«¯å¯¹è±¡
    :param commit: æ˜¯å¦æäº¤

    :return: æ¸¸æ ‡ å’Œ åˆ é™¤çš„æ•°æ®é‡
    """
    sql = """\
SELECT data.id
FROM data LEFT JOIN data AS data2 ON (data.id = data2.parent_id)
WHERE data.is_dir AND data2.id IS NULL
"""
    na_ids = filter_na_ids(client, (row[0] for row in con.execute(sql)))
    return delete_items(con, na_ids, commit=commit)


def update_path(
    con: Connection | Cursor, 
    /, 
    root_id: int = 0, 
    commit: bool = True, 
) -> tuple[Cursor, int, int]:
    """ä»¥ `dir` è¡¨ä¸ºå‡†ï¼Œå’Œ `data` è¡¨æ¯”å¯¹ï¼Œæ‰¾å‡ºæ‰€æœ‰ "name" æˆ– "parent_id" ä¸åŒçš„ç›®å½•ï¼Œç„¶åæ‰¹é‡æ›´æ–° `data` è¡¨ä¸­çš„æ•°æ®

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param root_id: æ ¹ç›®å½•çš„ idï¼Œå¦‚æœæ­¤ id > 0ï¼Œåˆ™å‡¡æ˜¯ä¹‹å‰åœ¨æ­¤ç›®å½•ä¸­ï¼Œå°†åœ¨æ›´æ–°åä¸åœ¨çš„ï¼Œéƒ½è¦è¢«åˆ é™¤
    :param commit: æ˜¯å¦æäº¤

    :return: 3 å…ƒç»„ï¼Œæ¸¸æ ‡ã€æ›´æ–°çš„æ•°æ®é‡ã€åˆ é™¤çš„æ•°æ®é‡
    """
    if isinstance(con, Connection):
        cur = con.cursor()
    else:
        cur = con
        con = cur.connection
    if root_id > 0:
        sql = "SELECT path FROM data WHERE id=?"
        row = cur.execute(sql, (root_id,)).fetchone()
        if row is None:
            root_id = 0
        else:
            root = row[0] + "/"
            root_new = get_dir_path(root_id) + "/"
    else:
        root_id = 0
    sql = """\
SELECT id, data.path, dir.mtime
FROM data JOIN dir USING (id)
WHERE data.name != dir.name OR data.parent_id != dir.parent_id
ORDER BY path DESC
"""
    updated = 0
    deleted = 0
    for cid, path, mtime in cur.execute(sql):
        name, pid = ID_TO_DIRNODE[cid]
        path_new = get_dir_path(cid)
        if root_id and path.startswith(root) and not path_new.startswith(root_new):
            cur.execute("DELETE FROM data WHERE id=? OR path LIKE ? || '/%'", (cid, path))
            deleted += cur.rowcount
        else:
            cur.execute("UPDATE data SET name=?, parent_id=?, path=?, mtime=? WHERE id=?", (name, pid, path_new, mtime, cid))
            cur.execute("UPDATE data SET path = ? || SUBSTR(path, ?) WHERE path LIKE ? || '/%'", (path_new, len(path) + 1, path))
            updated += cur.rowcount + 1
    if commit:
        con.commit()
    return cur, updated, deleted


def load_id_to_dirnode(con: Connection | Cursor, /):
    """æŠŠ `dir` è¡¨çš„æ•°æ®åŠ è½½åˆ°å…¨å±€å˜é‡ `ID_TO_DIRNODE` ä¸­

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    """
    sql = "SELECT id, name, parent_id FROM dir"
    for id, name, parent_id in con.execute(sql):
        ID_TO_DIRNODE[id] = DirNodeTuple((name, parent_id))


def update_id_to_dirnode(
    con: Connection | Cursor, 
    /, 
    client: P115Client, 
):
    """ä»ç½‘ä¸Šå¢é‡æ‹‰å–ç›®å½•æ•°æ®ï¼Œå¹¶æ›´æ–°åˆ° `dir` è¡¨å’Œå…¨å±€å˜é‡ `ID_TO_DIRNODE` ä¸­

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param client: 115 ç½‘ç›˜å®¢æˆ·ç«¯å¯¹è±¡
    """
    sql = "SELECT COALESCE(MAX(mtime), 0) FROM dir"
    mtime, = con.execute(sql).fetchone()
    data: list[dict] = list(takewhile(lambda attr: attr["mtime"] > mtime, iter_stared_dirs(
        client, 
        order="user_utime", 
        asc=0, 
        first_page_size=32, 
        id_to_dirnode=ID_TO_DIRNODE, 
        normalize_attr=normalize_dir_attr, 
    )))
    if data:
        insert_dir_items(con, data)


# TODO: å¦‚æœå‘ç”Ÿ id é‡å¤ï¼Œä½† count æ²¡å˜ï¼Œåˆ™å¹¶ä¸æŠ¥é”™ï¼Œä¼šä¸¢å¼ƒé‡å¤çš„ id çš„æ•°æ®ï¼Œç„¶åè·³è¿‡è€Œä¸è¿”å›ï¼Œå¹¶å¢åŠ è®¡æ•°å™¨ï¼Œç­‰æ‹‰å–å®Œåï¼Œä»å¤´éƒ¨å¼€å§‹å†å–ä¸€æ¬¡ï¼ˆæ¯å–å‡ºä¸€ä¸ªæœªè§åˆ°è¿‡çš„å…ƒç´ ï¼Œè®¡æ•°å™¨å‡ 1ï¼Œç›´åˆ°è®¡æ•°å™¨ä¸º 0ï¼‰
# TODO: æ¯æ¬¡éƒ½è¦è®°å½•ä¸Šä¸€æ¬¡çš„å¤´éƒ¨å…ƒç´ æ˜¯å“ªä¸ªï¼Œå› ä¸ºå¯èƒ½åå¤è¦ä»å¤´éƒ¨å¼€å§‹ï¼Œå»è¿½æ›´ï¼Œç›´åˆ°æŠŠæ‰€æœ‰æ›´æ–°éƒ½æ‰¾å…¨ï¼ˆå¦‚æœæ‰¾åˆ°ä¸Šæ¬¡çš„å¤´éƒ¨æ—¶ï¼Œæœªé‡åˆ°é‡å¤idï¼Œè€Œè®¡æ•°å™¨ä¸ä¸º 0ï¼Œåˆ™æŠ¥é”™ï¼‰
def iterdir(
    client: P115Client, 
    id: int = 0, 
    /, 
    page_size: int = 10_000, 
    first_page_size: None | int = None, 
    payload: dict = {}, 
) -> tuple[int, list[dict], dict[int, dict], Iterator[dict]]:
    """æ‹‰å–ä¸€ä¸ªç›®å½•ä¸­çš„æ–‡ä»¶æˆ–ç›®å½•çš„æ•°æ®

    :param client: 115 ç½‘ç›˜å®¢æˆ·ç«¯å¯¹è±¡
    :param id: ç›®å½•çš„ id
    :param page_size: åˆ†é¡µå¤§å°ï¼Œå¦‚æœ <= 0ï¼Œåˆ™ç”¨ 10_000
    :param first_page_size: é¦–æ¬¡æ‹‰å–çš„åˆ†é¡µå¤§å°ï¼Œå¦‚æœä¸º None æˆ–è€… <= 0ï¼Œåˆ™ç”¨ `page_size`
    :param payload: å…¶å®ƒæŸ¥è¯¢å‚æ•°

    :return: 4 å…ƒç»„ï¼Œåˆ†åˆ«æ˜¯

        1. æ€»æ•°
        2. ç¥–å…ˆèŠ‚ç‚¹çš„ç®€ç•¥ä¿¡æ¯ï¼ˆä¸å«æ ¹ç›®å½•ï¼‰
        3. å·²ç»æ‹‰å–çš„æ–‡ä»¶æˆ–ç›®å½•çš„æ•°æ®ï¼Œkey æ˜¯æ–‡ä»¶æˆ–ç›®å½•çš„ idï¼Œvalue æ˜¯ç›¸åº”çš„æ•°æ®
        4. è¿­ä»£å™¨ï¼Œç”¨æ¥è·å–æ•°æ®
    """
    if page_size <= 0:
        page_size = 10_000
    if not first_page_size or page_size <= 0:
        first_page_size = page_size
    payload = {
        "asc": 0, "cid": id, "custom_order": 1, "fc_mix": 1, "limit": first_page_size, 
        "show_dir": 1, "o": "user_utime", "offset": 0, **payload, 
    }
    fs_files = client.fs_files
    count = -1
    ancestors: list[dict] = []
    seen: dict[int, dict] = {}
    def get_files():
        nonlocal count
        resp = check_response(fs_files(payload, request=request))
        if int(resp["path"][-1]["cid"]) != id:
            if count < 0:
                raise NotADirectoryError(ENOTDIR, f"not a dir or deleted: cid={id}")
            else:
                raise FileNotFoundError(ENOENT, f"no such dir: cid={id}")
        ancestors[:] = (
            {"id": int(info["cid"]), "parent_id": int(info["pid"]), "name": info["name"]} 
            for info in resp["path"][1:]
        )
        if count < 0:
            count = resp["count"]
        elif count != resp["count"]:
            raise BusyOSError(EBUSY, f"detected count changes during iteration: cid={id}")
        return resp
    resp = get_files()
    def iterate():
        nonlocal resp
        offset = 0
        payload["limit"] = page_size
        while True:
            for attr in map(normalize_attr, resp["data"]):
                if attr["id"] in seen:
                    raise BusyOSError(
                        EBUSY, 
                        f"duplicate id found, means that some unpulled items have been updated: cid={id}", 
                    )
                seen[attr["id"]] = attr
                yield attr
            offset += len(resp["data"])
            if offset >= count:
                break
            payload["offset"] = offset
            resp = get_files()
    return count, ancestors, seen, iterate()


def diff_dir(
    con: Connection | Cursor, 
    client: P115Client, 
    id: int = 0, 
    /, 
    tree: bool = False, 
) -> tuple[list[int], list[dict]]:
    """æ‹‰å–æ•°æ®ï¼Œç¡®å®šå“ªäº›è®°å½•éœ€è¦åˆ é™¤æˆ–æ›´æ›¿

    :param con: æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
    :param client: 115 ç½‘ç›˜å®¢æˆ·ç«¯å¯¹è±¡
    :param id: ç›®å½•çš„ id
    :param tree: å¦‚æœä¸º Trueï¼Œåˆ™æ¯”å¯¹ç›®å½•æ ‘ï¼Œä½†ä»…å¯¹æ–‡ä»¶ï¼Œå³å¶å­èŠ‚ç‚¹ï¼Œå¦‚æœä¸º Falseï¼Œåˆ™æ¯”å¯¹æ‰€æœ‰ç›´æ¥å­èŠ‚ç‚¹ï¼ŒåŒ…æ‹¬æ–‡ä»¶å’Œç›®å½•

    :return: 2 å…ƒç»„ï¼Œ1) å¾…åˆ é™¤çš„ id åˆ—è¡¨ï¼Œ2) å¾…æ›´æ›¿çš„æ•°æ®åˆ—è¡¨
    """
    stored: dict[int, set[int]] = select_mtime_groups(con, id, tree=tree)
    n = sum(map(len, stored.values()))
    upsert_list: list[dict] = []
    delete_list: list[int] = []
    dirs: list[dict] = []
    upsert_add = upsert_list.append
    dirs_add = dirs.append
    if tree:
        count, ancestors, seen, data_it = iterdir(client, id, first_page_size=128 if n else 0, payload={"type": 99})
    else:
        count, ancestors, seen, data_it = iterdir(client, id, first_page_size=1 if n else 0)
    result = delete_list, upsert_list
    try:
        if not n:
            upsert_list += data_it
            return result
        it = iter(stored.items())
        his_mtime, his_ids = next(it)
        for attr in data_it:
            if attr["is_dir"]:
                dirs_add(attr)
            cur_id = attr["id"]
            cur_mtime = attr["mtime"]
            while his_mtime > cur_mtime:
                delete_list += his_ids - seen.keys()
                n -= len(his_ids)
                if not n:
                    upsert_add(attr)
                    upsert_list += data_it
                    return result
                his_mtime, his_ids = next(it)
            if his_mtime == cur_mtime:
                if cur_id in his_ids:
                    n -= 1
                    if count - len(seen) == n:
                        return result
                    his_ids.remove(cur_id)
                else:
                    upsert_add(attr)
            else:
                upsert_add(attr)
        for _, his_ids in it:
            delete_list += his_ids - seen.keys()
        return result
    finally:
        if ancestors:
            insert_dir_incomplete_items(con, ancestors)
        if dirs:
            insert_dir_items(con, dirs)


def updatedb_one(
    client: str | P115Client, 
    dbfile: None | str | Connection | Cursor = None, 
    id: int = 0, 
    /, 
):
    """
    """
    if isinstance(client, str):
        client = P115Client(client, check_for_relogin=True)
    if not dbfile:
        dbfile = f"115-{client.user_id}.db"
    if isinstance(dbfile, (Connection, Cursor)):
        con = dbfile
        start = time()
        try:
            to_delete, to_replace = diff_dir(con, client, id)
            with transaction(con):
                if to_delete:
                    delete_items(con, to_delete, commit=False)
                if to_replace:
                    ensure_attr_path(client, to_replace, with_path=True, id_to_dirnode=ID_TO_DIRNODE)
                    insert_items(con, to_replace, commit=False)
                _, updated, deleted = update_path(con, commit=False)
        except BaseException as e:
            logger.exception("[\x1b[1;31mFAIL\x1b[0m] %s", id)
            if isinstance(e, (FileNotFoundError, NotADirectoryError)):
                delete_items(con, id)
            raise
        else:
            logger.info(
                "[\x1b[1;32mGOOD\x1b[0m] %s, upsert: %d, delete: %d, update_path: %d, cost: %.6f s", 
                id, 
                len(to_replace), 
                len(to_delete) + deleted, 
                updated, 
                time() - start, 
            )
    else:
        with connect(dbfile, uri=dbfile.startswith("file:")) as con:
            initdb(con)
            load_id_to_dirnode(con)
            updatedb_one(client, con, id)


# TODO: æ–‡ä»¶å¦‚æœè¢«ç§»åŠ¨ä½ç½®ï¼Œå¹¶ä¸”è¿˜åœ¨ä¸€ä¸ªæ ¹ç›®å½•ä¹‹ä¸‹ï¼Œç”±æ­¤å®ƒè‡ªå·±çš„ mtime ä¸å˜ï¼Œè¿™è¦æ€ä¹ˆå¤„ç†ï¼Ÿæˆ–è®¸éœ€è¦ç»“åˆ 115 äº‹ä»¶
def updatedb_tree(
    client: str | P115Client, 
    dbfile: None | str | Connection | Cursor = None, 
    id: int = 0, 
    /, 
    no_dir_moved: bool = False, 
):
    """
    """
    if isinstance(client, str):
        client = P115Client(client, check_for_relogin=True)
    if not dbfile:
        dbfile = f"115-{client.user_id}.db"
    if isinstance(dbfile, (Connection, Cursor)):
        con = dbfile
        start = time()
        try:
            to_delete, to_replace = diff_dir(con, client, id, tree=True)
            if not no_dir_moved:
                update_id_to_dirnode(con, client)
                no_dir_moved = True
            if to_delete:
                # æ‰¾å‡ºæ‰€æœ‰å¾…åˆ é™¤è®°å½•çš„ç¥–å…ˆèŠ‚ç‚¹ idï¼Œå¹¶æ›´æ–°å®ƒä»¬çš„ mtime
                all_pids: set[int] = set()
                pids: Collection[int] = to_delete
                while pids := [id for id in select_parent_ids(con, pids) if id != 0 and id not in all_pids]:
                    all_pids.update(pids)
                if all_pids:
                    update_desc(client, all_pids)
                    no_dir_moved = False
                # æŠŠæ‰€æœ‰æ— æ•ˆçš„ id æ·»åŠ åˆ°å¾…åˆ é™¤åˆ—è¡¨
                to_delete += filter_na_ids(client, all_pids)
            if to_replace:
                # æ‰¾å‡ºæ‰€æœ‰å¾…æ›´æ–°è®°å½•çš„ç¥–å…ˆèŠ‚ç‚¹ idï¼Œå¹¶æ›´æ–°å®ƒä»¬çš„ mtime
                all_pids = set()
                pids = {ppid for attr in to_replace if (ppid := attr["parent_id"])}
                while pids:
                    all_pids |= pids
                    if find_ids := pids - ID_TO_DIRNODE.keys():
                        update_star(client, find_ids)
                        update_desc(client, pids)
                        update_id_to_dirnode(con, client)
                        no_dir_moved = True
                    else:
                        update_desc(client, pids)
                        no_dir_moved = False
                    pids = {ppid for pid in pids if (ppid := ID_TO_DIRNODE[pid][1]) and ppid not in all_pids}
            if not no_dir_moved:
                update_id_to_dirnode(con, client)
            if to_replace and all_pids:
                # æŠŠæ‰€æœ‰ç›¸å…³çš„ç›®å½• id æ·»åŠ åˆ°å¾…æ›´æ›¿åˆ—è¡¨
                to_replace += select_items_from_dir(con, all_pids)
                ensure_attr_path(client, to_replace, id_to_dirnode=ID_TO_DIRNODE)
            with transaction(con):
                if to_delete:
                    delete_items(con, to_delete, commit=False)
                if to_replace:
                    insert_items(con, to_replace, commit=False)
                _, updated, deleted = update_path(con, root_id=id, commit=False)
        except BaseException as e:
            logger.exception("[\x1b[1;31mFAIL\x1b[0m] %s", id)
            if isinstance(e, (FileNotFoundError, NotADirectoryError)):
                delete_items(con, id)
            raise
        else:
            logger.info(
                "[\x1b[1;32mGOOD\x1b[0m] %s, upsert: %d, delete: %d, update_path: %d, cost: %.6f s", 
                id, 
                len(to_replace), 
                len(to_delete) + deleted, 
                updated, 
                time() - start, 
            )
    else:
        with connect(dbfile, uri=dbfile.startswith("file:")) as con:
            initdb(con)
            load_id_to_dirnode(con)
            updatedb_tree(client, con, id, no_dir_moved=no_dir_moved)


def updatedb(
    client: str | P115Client, 
    dbfile: None | str | Connection | Cursor = None, 
    top_dirs: int | str | Iterable[int | str] = 0, 
    auto_splitting_threshold: int = 20_0000, 
    auto_splitting_statistics_timeout: None | float = 3, 
    no_dir_moved: bool = False, 
    recursive: bool = True, 
    clean: bool = False, 
):
    """
    """
    if isinstance(client, str):
        client = P115Client(client, check_for_relogin=True)
    if not dbfile:
        dbfile = f"115-{client.user_id}.db"
    if (auto_splitting_statistics_timeout is None or 
        isnan(auto_splitting_statistics_timeout) or 
        isinf(auto_splitting_statistics_timeout) or 
        auto_splitting_statistics_timeout <= 0
    ):
        auto_splitting_statistics_timeout = None
    if isinstance(dbfile, (Connection, Cursor)):
        con = dbfile
        seen: set[int] = set()
        seen_add = seen.add
        dq: deque[int] = deque()
        push, pop = dq.append, dq.popleft
        if isinstance(top_dirs, int):
            top_ids: Collection[int] = (top_dirs,)
        elif isinstance(top_dirs, str):
            top_dir = normalize_path(top_dirs)
            if isinstance(top_dir, int):
                top_ids = (top_dir,)
            else:
                try:
                    resp = check_response(client.fs_dir_getid(top_dir, request=request))
                    if not resp["id"]:
                        return
                    top_ids = (int(resp["id"]),)
                except:
                    logger.exception("[\x1b[1;31mFAIL\x1b[0m] %r", top_dirs)
                    return
        else:
            top_ids = set()
            add_id = top_ids.add
            for top_dir in top_dirs:
                if isinstance(top_dir, int):
                    add_id(top_dir)
                else:
                    top_dir = normalize_path(top_dir)
                    if isinstance(top_dir, int):
                        add_id(top_dir)
                    else:
                        try:
                            resp = check_response(client.fs_dir_getid(top_dir, request=request))
                            if not resp["id"]:
                                continue
                            add_id(int(resp["id"]))
                        except:
                            logger.exception("[\x1b[1;31mFAIL\x1b[0m] %r", top_dir)
                            continue
            if not top_ids:
                return
        dq.extend(top_ids)
        while dq:
            id = pop()
            if id in seen:
                logger.warning("[\x1b[1;33mSKIP\x1b[0m]", id)
                continue
            if auto_splitting_threshold <= 0:
                need_to_split_tasks = True
            else:
                if id == 0:
                    resp = check_response(client.fs_space_summury(request=request))
                    count = sum(v["count"] for k, v in resp["type_summury"].items() if k.isupper())
                else:
                    try:
                        resp = client.fs_category_get(id, timeout=auto_splitting_statistics_timeout, request=request)
                        if not resp:
                            seen_add(id)
                            continue
                        check_response(resp)
                        count = int(resp["count"])
                    except ReadTimeoutError:
                        count = float("inf")
                need_to_split_tasks = count > auto_splitting_threshold
            try:
                if not recursive or need_to_split_tasks:
                    updatedb_one(client, con, id)
                else:
                    updatedb_tree(client, con, id, no_dir_moved=no_dir_moved)
            except (FileNotFoundError, NotADirectoryError):
                pass
            except BusyOSError:
                logger.warning("[\x1b[1;34mREDO\x1b[0m] %s", id)
                push(id)
            else:
                seen_add(id)
                if recursive and need_to_split_tasks:
                    dq.extend(select_subdir_ids(con, id))
        if clean and top_ids:
            delete_na_dirs(con, client)
            delete_dangling_items(con)
    else:
        with connect(dbfile, uri=dbfile.startswith("file:")) as con:
            initdb(con)
            load_id_to_dirnode(con)
            updatedb(
                client, 
                con, 
                top_dirs=top_dirs, 
                auto_splitting_threshold=auto_splitting_threshold, 
                auto_splitting_statistics_timeout=auto_splitting_statistics_timeout, 
                no_dir_moved=no_dir_moved, 
                recursive=recursive, 
                clean=clean, 
            )
            if clean:
                con.execute("PRAGMA wal_checkpoint;")
                con.execute("VACUUM;")


if __name__ == "__main__":
    if args.cookies:
        cookies = args.cookies
    else:
        from pathlib import Path

        if args.cookies_path:
            cookies = Path(args.cookies_path).absolute()
        else:
            for path in (
                Path("./115-cookies.txt").absolute(), 
                Path("~/115-cookies.txt").expanduser(), 
                Path(__file__).parent / "115-cookies.txt", 
            ):
                if path.is_file():
                    cookies = path
            else:
                cookies = Path("~/115-cookies.txt").expanduser()

    client = P115Client(cookies, check_for_relogin=True)
    if not client.login_status():
        client.cookies = P115Client.login_with_qrcode("qandroid")["data"]["cookie"]

    updatedb(
        client, 
        dbfile=args.dbfile, 
        auto_splitting_threshold=args.auto_splitting_threshold, 
        auto_splitting_statistics_timeout=args.auto_splitting_statistics_timeout, 
        no_dir_moved=args.no_dir_moved, 
        recursive=not args.not_recursive, 
        top_dirs=args.top_dirs or 0, 
        clean=args.clean, 
    )

# NOTE: ä»¥ä¸‹è¿™äº›æ˜¯å¾…å®ç°çš„è®¾æƒ³ ğŸ‘‡
# TODO: ä½œä¸ºæ¨¡å—æä¾›ï¼Œå…è®¸å…¨é‡æ›´æ–°(updatedb)å’Œå¢é‡æ›´æ–°(updatedb_one)ï¼Œä½†åªå…è®¸åŒæ—¶æœ€å¤šä¸€ä¸ªå†™å…¥ä»»åŠ¡
# TODO: å¯ä»¥èµ·ä¸€ä¸ªæœåŠ¡ï¼Œå…¶å®ƒçš„ç¨‹åºï¼Œå¯ä»¥å‘é€è¯»å†™ä»»åŠ¡è¿‡æ¥ï¼Œæ•°æ®åº“å¯ä»¥ä»¥ fuse æˆ– webdav å±•ç¤º
# TODO: æ”¯æŒå¤šä¸ªä¸åŒç™»å½•è®¾å¤‡å¹¶å‘
# TODO: æ”¯æŒåŒä¸€ä¸ª cookies å¹¶å‘å› å­ï¼Œé»˜è®¤å€¼ 1
# TODO: ä½¿ç”¨åç¨‹è¿›è¡Œå¹¶å‘ï¼Œè€Œéå¤šçº¿ç¨‹
# TODO: å¦‚æœè¯·æ±‚è¶…æ—¶ï¼Œåˆ™éœ€è¦è¿›è¡Œé‡è¯•
# TODO: sqlite çš„æ•°æ®åº“äº‹åŠ¡å’Œå†™å…¥ä¼šè‡ªåŠ¨åŠ é”ï¼Œå¦‚æœæœ‰å¤šä¸ªç¨‹åºåœ¨å¹¶å‘ï¼Œåˆ™å¯ä»¥ç­‰å¾…é”ï¼Œéœ€è¦ä¸€ä¸ªè¶…æ—¶æ—¶é—´å’Œé‡è¯•æ¬¡æ•°
# TODO: iterdir å‡½æ•°æ”¯æŒå¹¶å‘

# TODO: å¦‚æœç›¸åŒ parent_id ä¸‹ï¼Œæœ‰åŒåçš„ç›®å½•ï¼Œåˆ™è¯´æ˜æœ‰å†²çªï¼Œéœ€è¦åˆ æ‰æ—§æœ‰çš„ï¼ˆä½†åˆç”±äºå¯èƒ½æ˜¯å‘ç”Ÿäº†ç§»åŠ¨ï¼Œå¦‚æœç›´æ¥åˆ é™¤ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸‹æ¬¡ä¼šé‡æ–°æ‹‰å–å¤§é‡æ•°æ®ï¼‰
#       parent_id, name, is_dir=1
# TODO: å¢åŠ å­å‘½ä»¤ï¼Œå¯ä»¥åˆ é™¤ç›®å½•æ ‘ï¼ˆé€šè¿‡æ ¹ id æˆ–è€…æ ¹è·¯å¾„ï¼‰
# TODO: å¦‚æœä¸€ä¸ªæ–‡ä»¶å¤¹è¢«ç§»åŠ¨ï¼Œé‚£ä¹ˆå®ƒçš„æ›´æ–°æ—¶é—´ä¸ä¼šå˜ï¼Œåªæ˜¯å®ƒçš„ä¸Šçº§ id çš„æ›´æ–°æ—¶é—´ä¼šå˜ï¼Œå› æ­¤å¿…è¦æ—¶ï¼Œè¿˜æ˜¯éœ€è¦ç»“åˆ 115 æ›´æ–°äº‹ä»¶

# TODO: å¢åŠ ä¸€ä¸ªé€‰é¡¹ï¼Œå…è®¸å¯¹æ•°æ®è¿›è¡Œå…¨é‡è€Œä¸æ˜¯å¢é‡æ›´æ–°ï¼Œè¿™æ ·å¯ä»¥é¿å…ä¸€äº›é—®é¢˜
# TODO: å¦‚æœæŸ¥è¯¢çš„æŸä¸ª id ä¸å­˜åœ¨ï¼Œå°±æŠŠè¿™ä¸ª id çš„åœ¨æ•°æ®åº“çš„æ•°æ®ç»™åˆ é™¤
